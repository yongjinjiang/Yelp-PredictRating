{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Yelp_Dataset - Part_II_NLP.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongjinjiang/Yelp-PredictRating/blob/master/Yelp_Dataset_Part_II_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diRkCZON4axO",
        "colab_type": "text"
      },
      "source": [
        "## In this notebook, we would build three models: \n",
        "- Naive Bayes, \n",
        "- Logistic regression,\n",
        "- random forest \n",
        "We will compare their performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx2yjkh93CNh",
        "colab_type": "text"
      },
      "source": [
        "# Yelp Data Challenge \n",
        "## Part II - NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBJVXgu531Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVz_KE44XJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Yelp-NLP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQjXWycb3CNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rXahzrm3CNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./last_3_years_restaurant_reviews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT8o9sx93CNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXg9FFuE3CN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID5ftUwg3CN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review_cnt'] = df.groupby(['business_id'])['review_id'].transform('count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN5w2fk93CN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review_cnt'].quantile(q=[0.5,0.75,0.90])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHjHSu_d3COC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB_5woLk3COF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review_cnt'].value_counts().plot.hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFngTBK33COJ",
        "colab_type": "text"
      },
      "source": [
        "### Define feature variables, here is the text of the review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Lnzrjw3COK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = df['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WCoDXec3COQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get non na indx\n",
        "indx = pd.notnull(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUlpUkHf3COU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = documents[indx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5efHAL13COZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
        "documents = documents.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflXnzTY3COc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
        "documents[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G2in24L3COf",
        "colab_type": "text"
      },
      "source": [
        "### Define target variable (any categorical variable that may be meaningful)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdXXi02e3COg",
        "colab_type": "text"
      },
      "source": [
        "#### For example, I am interested in perfect (5 stars) and imperfect (1-4 stars) rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEcKRGkf3COh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxlzifpQ3COo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking summary statisics for avg_star and stars\n",
        "var_list = [u'avg_stars',u'stars',u'cool',u'funny',u'useful']\n",
        "df[var_list].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb2K8amJ3COr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a column and take the values, save to a variable named \"target\"\n",
        "# tried cutoff >3, pos/neg around 7:3; cutoff > 4, pos/neg around 1:1\n",
        "#condition1 = (df['stars']>3)\n",
        "#df_reduced.drop(['is_pos_review'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XFwgi7w3COu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['is_pos_review'] = (df['stars']>4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-4_Kj7_3COy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['is_pos_review'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E7THHHF3CO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = df['is_pos_review'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL-cgOwV3CO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = target[indx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-OmorU3CO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D60ige3p3CPB",
        "colab_type": "text"
      },
      "source": [
        "#### You may want to look at the statistic of the target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o28TJrP3CPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To be implemented\n",
        "#checking positive reviews's avg_stars rating\n",
        "df[df['is_pos_review']==True]['avg_stars'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlnUyZ7G3CPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents.size,target.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHZBZyDo3CPI",
        "colab_type": "text"
      },
      "source": [
        "## Let's create training dataset and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Zj5RKt3CPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cross_validation import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR9m1mE23CPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Documents is your X, target is your y\n",
        "# Now split the data to training set and test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYU3WRd63CPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split to documents_train, documents_test, target_train, target_test\n",
        "# using test_size 0.8 to reduce the training size due to computational cost\n",
        "documents_train,documents_test,target_train,target_test = train_test_split(documents,target,test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5AftUy3CPT",
        "colab_type": "text"
      },
      "source": [
        "## Let's get NLP representation of the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W8qoTp93CPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GJcyVPF3CPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create TfidfVectorizer, and name it vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000,min_df=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg5QvdAq3CPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model with your training data\n",
        "x_train = vectorizer.fit_transform(documents_train).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhzQkE6Z3CPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the vocab of your tfidf\n",
        "features_name = vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61nQfoUy3CPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the trained model to transform your test data\n",
        "x_test = vectorizer.transform(documents_test).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4TRNMum3CPr",
        "colab_type": "text"
      },
      "source": [
        "##### using 2-gram of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAG3gQ73CPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create TfidfVectorizer, and name it vectorizer\n",
        "vectorizer2 = TfidfVectorizer(stop_words='english',max_features=5000,min_df=1,ngram_range=(1, 2))\n",
        "x_train2 = vectorizer2.fit_transform(documents_train).toarray()\n",
        "features_name2 = vectorizer2.get_feature_names()\n",
        "x_test2 = vectorizer2.transform(documents_test).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N1j1FgI3CPu",
        "colab_type": "text"
      },
      "source": [
        "## Similar review search engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hvgKZYg3CPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We will need these helper methods pretty soon\n",
        "\n",
        "def get_top_values(lst, n, labels):\n",
        "    '''\n",
        "    INPUT: LIST, INTEGER, LIST\n",
        "    OUTPUT: LIST\n",
        "\n",
        "    Given a list of values, find the indices with the highest n values.\n",
        "    Return the labels for each of these indices.\n",
        "\n",
        "    e.g.\n",
        "    lst = [7, 3, 2, 4, 1]\n",
        "    n = 2\n",
        "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
        "    output: [\"cat\", \"pig\"]\n",
        "    '''\n",
        "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  # np.argsort by default sorts values in ascending order\n",
        "\n",
        "def get_bottom_values(lst, n, labels):\n",
        "    '''\n",
        "    INPUT: LIST, INTEGER, LIST\n",
        "    OUTPUT: LIST\n",
        "\n",
        "    Given a list of values, find the indices with the lowest n values.\n",
        "    Return the labels for each of these indices.\n",
        "\n",
        "    e.g.\n",
        "    lst = [7, 3, 2, 4, 1]\n",
        "    n = 2\n",
        "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
        "    output: [\"mouse\", \"rabbit\"]\n",
        "    '''\n",
        "    return [labels[i] for i in np.argsort(lst)[:n]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoyQmxcr3CP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's use cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMeiBcUR3CQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw an arbitrary review from test (unseen in training) documents\n",
        "sample_review = [documents_test[10]]\n",
        "sample_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOhvyipP3CQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the drawn review(s) to vector(s)\n",
        "vector_review = vectorizer.transform(sample_review).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QauS1WKL3CQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_review2 = vectorizer2.transform(sample_review).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcG8S2wO3CQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the similarity score(s) between vector(s) and training vectors\n",
        "similarity_scores = cosine_similarity(vector_review, x_test[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjFvYQ33CQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_scores2 = cosine_similarity(vector_review2,x_test2[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1zIOdjI3CQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_scores[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgOJaCZL3CQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's find top 5 similar reviews\n",
        "n = 5\n",
        "top_similar_review = get_top_values(similarity_scores[0], n, documents_test[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyc_KtEg3CQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 5\n",
        "top_similar_review2 = get_top_values(similarity_scores2[0],n,documents_test[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCoyKqr93CQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Our search query:')\n",
        "print(sample_review) # To be added"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hax5yMXd3CQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Most %s similar reviews:' % n)\n",
        "for i in range(len(top_similar_review)):\n",
        "    print ('top %s review:' % i)\n",
        "    print (top_similar_review[i])\n",
        " # To be added"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQamdzoe3CRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Most %s similar reviews:' % n)\n",
        "for i in range(len(top_similar_review2)):\n",
        "    print ('top %s review:' % i)\n",
        "    print (top_similar_review2[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh84Mk6G3CRF",
        "colab_type": "text"
      },
      "source": [
        "#### Q: Does the result make sense to you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmXNb2Sk3CRG",
        "colab_type": "text"
      },
      "source": [
        "A: The top5 reviews captures some key elements for the sample review, such as casino, service,decor, cleaning; but the cosine similarity doesn't catch the meaning of sentence very well. The sample review is a strong postive review using double negative formatting, however the top 5 reviews are more on the negative side of the hotels/casino.\n",
        "\n",
        "Fitting 2 models using different grams, the result of top 5 reviews seem to be unchanged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQIWnEkb3CRH",
        "colab_type": "text"
      },
      "source": [
        "## Classifying positive/negative review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4bEXkKb3CRJ",
        "colab_type": "text"
      },
      "source": [
        "### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKUzd5xU3CRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score,roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxfud4UU3CRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper method to print metric scores    \n",
        "def get_performance_metrics(y_train, y_train_pred, y_test, y_test_pred, threshold=0.5):\n",
        "    metric_names = ['AUC','Accuracy','Precision','Recall','f1-score']\n",
        "    metric_values_train = [roc_auc_score(y_train, y_train_pred),\n",
        "                    accuracy_score(y_train, y_train_pred>threshold),\n",
        "                    precision_score(y_train, y_train_pred>threshold),\n",
        "                    recall_score(y_train, y_train_pred>threshold),\n",
        "                    f1_score(y_train, y_train_pred>threshold)\n",
        "                   ]\n",
        "    metric_values_test = [roc_auc_score(y_test, y_test_pred),\n",
        "                    accuracy_score(y_test, y_test_pred>threshold),\n",
        "                    precision_score(y_test, y_test_pred>threshold),\n",
        "                    recall_score(y_test, y_test_pred>threshold),\n",
        "                    f1_score(y_test, y_test_pred>threshold)\n",
        "                   ]\n",
        "    all_metrics = pd.DataFrame({'metrics':metric_names,\n",
        "                                'train':metric_values_train,\n",
        "                                'test':metric_values_test},columns=['metrics','train','test']).set_index('metrics')\n",
        "    print(all_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZncJXpZ43CRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helper function to plot roc curve\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc_curve(y_train, y_train_pred, y_test, y_test_pred):\n",
        "    roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n",
        "\n",
        "    roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr_train, tpr_train, color='green',\n",
        "             lw=lw, label='ROC Train (AUC = %0.4f)' % roc_auc_train)\n",
        "    plt.plot(fpr_test, tpr_test, color='darkorange',\n",
        "             lw=lw, label='ROC Test (AUC = %0.4f)' % roc_auc_test)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9znPE_j3CRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to train models\n",
        "# define function to perform train, test, and get model performance\n",
        "def train_test_model(clf, X_train, y_train, X_test, y_test):\n",
        "    # Fit a model by providing X and y from training set\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make prediction on the training data\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    p_train_pred = clf.predict_proba(X_train)[:,1]\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    p_test_pred = clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "    # print model results\n",
        "    get_performance_metrics(y_train, p_train_pred, y_test, p_test_pred)\n",
        "    plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL0HrQwG3CRc",
        "colab_type": "text"
      },
      "source": [
        "### Naive-Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-qi2r613CRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a Naive-Bayes Classifier\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Build a Naive-Bayes Classifier\n",
        "clf_nb = MultinomialNB()\n",
        "\n",
        "clf_nb.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T73vLzb83CRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get score for training set & test set\n",
        "train_test_model(clf_nb, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBwT5J9-3CRh",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "310WyDoG3CRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a Logistic Regression Classifier\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_lrc = LogisticRegression()\n",
        "\n",
        "clf_lrc.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLYzjXPQ3CRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get score for training set & test set\n",
        "train_test_model(clf_lrc, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFBSBU1W3CRm",
        "colab_type": "text"
      },
      "source": [
        "#### Q: What are the key features(words) that make the positive prediction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBZRm1B73CRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's find it out by ranking\n",
        "n = 20\n",
        "get_top_values(clf_lrc.coef_[0], n, features_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48AX_1CA3CRp",
        "colab_type": "text"
      },
      "source": [
        "A: The top 20 words all positive adjective words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDd2Zw893CRq",
        "colab_type": "text"
      },
      "source": [
        "#### Q: What are the key features(words) that make the negative prediction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eujadm_93CRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's find it out by ranking\n",
        "n = 20\n",
        "get_bottom_values(clf_lrc.coef_[0], n, features_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n17m-gG-3CRy",
        "colab_type": "text"
      },
      "source": [
        "A: all the words are negative adjective words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8-JjAbi3CRy",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8362y7X3CRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a Random Forest Classifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf_rfc = RandomForestClassifier(max_depth=20, n_estimators=10, min_samples_leaf=100)\n",
        "\n",
        "clf_rfc.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrR1SWLA3CR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get score for training set & test set\n",
        "train_test_model(clf_rfc, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MyDQsVP3CR4",
        "colab_type": "text"
      },
      "source": [
        "#### Q: What do you see from the training score and the test score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl5lnzkq3CR5",
        "colab_type": "text"
      },
      "source": [
        "A: Comparing the three classifiers, random frost has lowest accuracy and AUC for both train/test data. For different metrics, the train metric and test metric are quite close."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYUfGH0H3CR5",
        "colab_type": "text"
      },
      "source": [
        "#### Q: Can you tell what features (words) are important by inspecting the RFC model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bVI03I3CR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 20\n",
        "get_top_values(clf_rfc.feature_importances_, n, features_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCV5Ud1M3CR9",
        "colab_type": "text"
      },
      "source": [
        "Compared to Logistic regression results, top 20 features in RF are mix of positive words and negative words. it also have some neutral words like (hotel, money...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6MygevR3CR-",
        "colab_type": "text"
      },
      "source": [
        "## Additional Approach #1: Use cross validation to evaluate classifiers\n",
        "\n",
        "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfme-VEp3CSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# too slow, not used\n",
        "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score,KFold\n",
        "def get_scores(clf,X,y,num_folds=5):\n",
        "    metric_names = ['AUC','Accuracy','Precision','Recall','f1-score']\n",
        "    accuracy_score = cross_val_score(clf, X, y, cv=num_folds,scoring='accuracy')\n",
        "    auc_score = cross_val_score(clf, X, y, cv=num_folds,scoring='roc_auc')\n",
        "    pre_score = cross_val_score(clf, X, y, cv=num_folds,scoring='precision')\n",
        "    recall_score = cross_val_score(clf, X, y, cv=num_folds,scoring='recall')\n",
        "    f1_score = cross_val_score(clf, X, y, cv=num_folds,scoring='f1')  \n",
        "    metric_mean = [auc_score.mean(),accuracy_score.mean(),pre_score.mean(),recall_score.mean(),f1_score.mean()]\n",
        "    metric_std = [auc_score.std(),accuracy_score.std(),pre_score.std(),recall_score.std(),f1_score.std()]\n",
        "    all_metrics = pd.DataFrame({'metrics':metric_names,\n",
        "                                'cv_mean':metric_mean,\n",
        "                                'cv_std':metric_std},columns=['metrics','cv_mean','cv_std']).set_index('metrics')\n",
        "    print(all_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdouPup73CSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_validate\n",
        "def get_scores(clf,X,y,num_folds=5):\n",
        "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "               'prec': 'precision',\n",
        "                'roc_auc': make_scorer(roc_auc_score),\n",
        "                'recall': make_scorer(recall_score),\n",
        "                'f1': make_scorer(f1_score)}\n",
        "    cv_results = cross_validate(clf, X, y, scoring=scoring,cv=num_folds,return_train_score=False)\n",
        "    return cv_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJTHtwA3CSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_nb = get_scores(clf_nb,x_train,target_train,num_folds=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqZ53Klr3CSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_lrc = get_scores(clf_lrc,x_train,target_train,num_folds=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT2Q4_VL3CSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_rfc = get_scores(clf_rfc,x_train,target_train,num_folds=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RfSCuXH3CSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_nb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuwZL6ZF3CSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_lrc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEkCMNTD3CSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_metrics_rfc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1OAYIMX3CSX",
        "colab_type": "text"
      },
      "source": [
        "CV scores are lower than single train/test split, but the model performance ranking is consistent with previous results. Logistic regression classifier > naive bayes > random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdeGZ9yd3CSY",
        "colab_type": "text"
      },
      "source": [
        "## Additional Approach #2: Use grid methods to find best predictable classifier\n",
        "\n",
        "\n",
        "[sklearn grid search tutorial (with cross validation)](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
        "\n",
        "[sklearn grid search documentation (with cross validation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uGWEDKH3CSY",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXV88nr73CSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Choose the type of classifier. \n",
        "clf = LogisticRegression()\n",
        "\n",
        "# Choose some parameter combinations to try\n",
        "param_grid = {'penalty':['l1','l2'],\n",
        "               'C':[0.5,1,5,10],\n",
        "               'solver':['liblinear']\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = make_scorer(roc_auc_score)\n",
        "\n",
        "# Run the grid search\n",
        "# read theory\n",
        "grid_obj = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)\n",
        "grid_obj = grid_obj.fit(x_train, target_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "clf = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data. \n",
        "clf.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpECChYT3CSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train test model\n",
        "train_test_model(clf, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWgIG3Ep3CSf",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsF7d-PK3CSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Choose the type of classifier. \n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Choose some parameter combinations to try\n",
        "param_grid = {'alpha':[0.5,1,2]\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = make_scorer(roc_auc_score)\n",
        "\n",
        "# Run the grid search\n",
        "# read theory\n",
        "grid_obj = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)\n",
        "grid_obj = grid_obj.fit(x_train, target_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "clf = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data. \n",
        "clf.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfEookOa3CSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_model(clf, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocVWquVg3CSm",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7MaCRTm3CSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Choose the type of classifier. \n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Choose some parameter combinations to try\n",
        "param_grid = {'n_estimators': [100,200], \n",
        "              'max_features': ['auto'], \n",
        "              'criterion': ['gini'],\n",
        "              'max_depth': [8,16,32], \n",
        "              'min_samples_split': [5,10,20,60],\n",
        "              'min_samples_leaf': [2,5,10,20],\n",
        "              'n_jobs':[-1]\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = make_scorer(roc_auc_score)\n",
        "\n",
        "# Run the grid search\n",
        "# read theory\n",
        "grid_obj = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)\n",
        "grid_obj = grid_obj.fit(x_train, target_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "clf = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data. \n",
        "clf.fit(x_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bONmowI3CSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_model(clf, x_train, target_train, x_test, target_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_d2g4OL3CSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('hello world')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}